# Informe Equipo A (Bronze & Silver)   
# Entrega: Proyecto LIDL - Pipeline de Datos de Clientes

**Proyecto:** Ingesta y Limpieza de Datos de Clientes para Adquisici√≥n en Chile  
**Fecha de Entrega:** 01 de Diciembre, 2025  
**Responsables:**  
- William Moncada  
- Matias Osores  
- Catalina Fajardo  
- Dilan Ferreira  
- Jean Piere Trinidad  

**Destinatario:** Equipo de An√°lisis (Equipo B ‚Äì Silver)


> **Ingesta y Limpieza de Datos para Adquisici√≥n de Supermercado Chileno**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5.0-orange.svg)](https://spark.apache.org/)
[![GitHub Codespaces](https://img.shields.io/badge/GitHub-Codespaces-success.svg)](https://github.com/features/codespaces)

---

##  Tabla de Contenidos

- [Resumen Ejecutivo](#-resumen-ejecutivo)
- [¬øPor qu√© GitHub Codespaces?](#-por-qu√©-github-codespaces)
- [Arquitectura del Proyecto](#-arquitectura-del-proyecto)
- [Gu√≠a de Inicio R√°pido](#-gu√≠a-de-inicio-r√°pido)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Pipeline de Datos](#-pipeline-de-datos)
- [Verificaci√≥n de Resultados](#-verificaci√≥n-de-resultados)
- [Equipo](#-equipo)

---

##  Resumen Ejecutivo

Este proyecto implementa un **pipeline de datos** completo para procesar informaci√≥n de 500 clientes de un supermercado chileno candidato a adquisici√≥n por LIDL. Utiliza la **Arquitectura Medallion (Bronze ‚Üí Silver ‚Üí Gold)** para garantizar calidad y trazabilidad.
## 1.1 Workflow del Pipeline ETL

```mermaid
flowchart LR
    A[Usuario ejecuta python ingesta_pipeline.py] --> B[Inicializar Spark]

    B --> C[Leer CSV clientes_info.csv]
    B --> D[Leer TXT clientes_extra.txt]
    B --> E[Leer SQL clientes.sql]

    C --> F[Mapeo CSV]
    D --> G[Mapeo TXT]
    E --> H[Parsing SQL]

    F --> I[Uni√≥n Bronze]
    G --> I
    H --> I

    I --> J[Validaci√≥n de ID]
    J --> K[Escritura Bronze]

    K --> L[Leer Bronze]
    L --> M[Normalizar Texto]
    M --> N[Estandarizar Fechas]
    N --> O[Manejo de Nulos]
    O --> P[Selecci√≥n Final]
    P --> Q[Escritura Silver]
```

---

## 2. Arquitectura de Datos y Entorno de Trabajo

### 2.1 Arquitectura de Datos (Metodolog√≠a)

Se utiliz√≥ la **Arquitectura Medallion (Bronze ‚Üí Silver)** para garantizar trazabilidad y calidad.


```mermaid

flowchart LR
    subgraph HOST[Host Local o Codespace]
        SO[Sistema Operativo]
        PY[Python + Librer√≠as: PySpark, Pandas]
        PROC[Proceso ETL: ingesta_pipeline.py]
        SO --> PY --> PROC
    end

    subgraph RAW[Fuentes de Datos]
        CSV[clientes_info.csv]
        TXT[clientes_extra.txt]
        SQL[clientes.sql]
    end

    subgraph BRONZE[Capa Bronze - data/bronze/ventas]
        BR[Archivos Parquet Bronze]
    end

    subgraph SILVER[Capa Silver - data/silver/ventas]
        SI[Archivos Parquet Silver]
    end

    RAW --> PROC
    PROC --> BR
    BR --> PROC
    PROC --> SI
    SI --> CONS[Equipo Anal√≠tico o BI]
```

## 3. Estructura de Datos en Capa SILVER (Consumo Anal√≠tico)

### 3.1 Proceso de Limpieza y Estandarizaci√≥n

```mermaid
flowchart LR
    A[Leer Bronze] --> B[Filtrar IDs v√°lidos]
    B --> C[Normalizar Texto]
    C --> D[Estandarizar Fechas]
    D --> E[Manejo de Nulos]
    E --> F[Selecci√≥n Final]
    F --> G[Escritura Silver]
```

---
###  Datos Procesados

| Fuente | Formato | Registros | Contenido |
|--------|---------|-----------|-----------|
| `clientes.sql` | SQL INSERT | 500 | Datos demogr√°ficos (nombre, RUT, comuna, religi√≥n) |
| `clientes_info.csv` | CSV | 500 | M√©tricas de comportamiento (compras, permanencia, alimentaci√≥n) |
| `clientes_extra.txt` | TXT/CSV | 500 | Informaci√≥n de afiliaci√≥n (tipo servicio, fecha) |

### ‚úÖ Resultados

- **1,500 registros** ingestados correctamente
- **Bronze Layer**: Datos crudos en formato Parquet
- **Silver Layer**: Datos limpios y normalizados listos para an√°lisis
- **Pipeline automatizado** con validaciones y logging

---

##  ¬øPor qu√© GitHub Codespaces?

### Razones T√©cnicas

‚úÖ **Sin problemas de sistema operativo**: Funciona igual en Windows, Mac y Linux  
‚úÖ **Entorno pre-configurado**: Python, Java y Spark ya instalados  
‚úÖ **Sin instalaci√≥n local**: No requiere configurar tu m√°quina  
‚úÖ **Colaboraci√≥n instant√°nea**: Todo el equipo trabaja en el mismo entorno  
‚úÖ **Recursos garantizados**: 4 cores, 8GB RAM, 32GB storage  
‚úÖ **Gratis para uso educativo**: 60 horas/mes sin costo

### Alternativa Local

Si prefieres trabajar localmente, necesitas:
- Python 3.8+
- Java 11+ (requerido por PySpark)
- 4GB RAM m√≠nimo
- Git

---

## Arquitectura del Proyecto

### Arquitectura Medallion

```mermaid
flowchart LR
    subgraph Sources[üìÅ Fuentes de Datos]
        CSV[clientes_info.csv<br/>500 registros]
        TXT[clientes_extra.txt<br/>500 registros]
        SQL[clientes.sql<br/>500 registros]
    end

    subgraph Bronze[ü•â Bronze Layer - Datos Crudos]
        B1[clientes_extra_bronze.parquet]
        B2[clientes_info_bronze.parquet]
        B3[clientes_bronze.parquet]
    end

    subgraph Silver[ü•à Silver Layer - Datos Limpios]
        S1[clientes_extra_silver.parquet]
        S2[clientes_info_silver.parquet]
        S3[clientes_silver.parquet]
    end

    CSV --> B2
    TXT --> B1
    SQL --> B3

    B1 --> S1
    B2 --> S2
    B3 --> S3

    S1 --> Analytics[üìä An√°lisis<br/>Modelado<br/>BI]
    S2 --> Analytics
    S3 --> Analytics

    style Bronze fill:#cd7f32,stroke:#000,color:#fff
    style Silver fill:#c0c0c0,stroke:#000,color:#000
    style Analytics fill:#4CAF50,stroke:#000,color:#fff
```

### Workflow del Pipeline

```mermaid
flowchart TB
    Start([üöÄ Inicio]) --> Init[Inicializar Entorno Virtual]
    Init --> Ingest[üì• Ingesta Bronze Layer]
    
    Ingest --> V1{Validar<br/>Datos?}
    V1 -->|‚úÖ OK| Clean[üßπ Limpieza Silver Layer]
    V1 -->|‚ùå Error| Log1[üìù Log Error]
    
    Clean --> V2{Datos<br/>V√°lidos?}
    V2 -->|‚úÖ OK| Save[üíæ Guardar Silver]
    V2 -->|‚ùå Error| Log2[üìù Log Error]
    
    Save --> Report[üìä Generar Reporte]
    Report --> End([‚úÖ Completado])
    
    Log1 --> End
    Log2 --> End
    
    style Start fill:#4CAF50,color:#fff
    style End fill:#2196F3,color:#fff
    style V1 fill:#FF9800,color:#fff
    style V2 fill:#FF9800,color:#fff
```

---

##  Gu√≠a de Inicio R√°pido

### Opci√≥n 1: GitHub Codespaces (Recomendado) ‚≠ê

#### Paso 1: Abrir en Codespaces

1. Ve al repositorio: `[https://github.com/TrinidadJean/lidl_project]`
2. Click en el bot√≥n verde **"<> Code"**
3. Selecciona la pesta√±a **"Codespaces"**
4. Click en **"Create codespace on main"**
5. Espera 2-3 minutos mientras se configura el entorno

![Codespaces Setup](https://docs.github.com/assets/cb-138303/mw-1440/images/help/codespaces/new-codespace-button.webp)

#### Paso 2: Configurar Entorno Virtual

```bash
# El terminal se abrir√° autom√°ticamente
# Ejecuta estos comandos en orden:

# 1. Crear entorno virtual
python -m venv venv

# 2. Activar entorno virtual
source venv/bin/activate

# 3. Actualizar pip
pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt

# Verificar instalaci√≥n
pip list
```

#### Paso 3: Ejecutar Pipeline

```bash
# Ejecutar pipeline completo (Bronze + Silver)
python main.py
```

**Salida esperada:**
```
============================================================
üè™ PROYECTO LIDL - PIPELINE DE DATOS
============================================================

üì• ETAPA 1: Ingesta Bronze Layer
------------------------------------------------------------
‚úì clientes_extra.txt ingresado: 500 registros
‚úì clientes_info.csv ingresado: 500 registros
‚úì clientes.sql ingresado: 500 registros

üßπ ETAPA 2: Limpieza Silver Layer
------------------------------------------------------------
‚úì clientes_extra limpiado: 500 registros
‚úì clientes_info limpiado: 500 registros
‚úì clientes limpiado: 500 registros

============================================================
‚úÖ WORKFLOW COMPLETADO EXITOSAMENTE
============================================================
‚è±Ô∏è  Duraci√≥n total: 45.23 segundos
üìÅ Archivos Bronze: bronze/ventas/
üìÅ Archivos Silver: silver/ventas/
üìã Logs: logs/
============================================================
```

#### Paso 4: Verificar Resultados

```bash
# Ver estructura de archivos generados
tree bronze/ silver/

# Ver logs del proceso
cat logs/main_workflow.log

# Ver estad√≠sticas de ingesta
cat bronze/ventas/ingesta_stats.json
```

---

### Opci√≥n 2: Instalaci√≥n Local

#### Requisitos Previos

- Python 3.8 o superior
- Java 11 o superior (para PySpark)
- Git
- 4GB RAM m√≠nimo

#### Pasos

```bash
# 1. Clonar repositorio
git clone https://github.com/pconstancioteacher/lidl_project.git
cd lidl_project

# 2. Crear entorno virtual
python -m venv venv

# 3. Activar entorno virtual
# En Linux/Mac:
source venv/bin/activate
# En Windows (PowerShell):
venv\Scripts\Activate.ps1

# 4. Instalar dependencias
pip install -r requirements.txt

# 5. Ejecutar pipeline
python main.py
```

---

##  Estructura del Proyecto

```
lidl_project/
‚îú‚îÄ‚îÄ üìÅ bronze/                    # Capa Bronze (datos crudos)
‚îÇ   ‚îî‚îÄ‚îÄ ventas/
‚îÇ       ‚îú‚îÄ‚îÄ clientes_extra_bronze.parquet
‚îÇ       ‚îú‚îÄ‚îÄ clientes_info_bronze.parquet
‚îÇ       ‚îú‚îÄ‚îÄ clientes_bronze.parquet
‚îÇ       ‚îî‚îÄ‚îÄ ingesta_stats.json
‚îÇ
‚îú‚îÄ‚îÄ üìÅ silver/                    # Capa Silver (datos limpios)
‚îÇ   ‚îî‚îÄ‚îÄ ventas/
‚îÇ       ‚îú‚îÄ‚îÄ clientes_extra_silver.parquet
‚îÇ       ‚îú‚îÄ‚îÄ clientes_info_silver.parquet
‚îÇ       ‚îî‚îÄ‚îÄ clientes_silver.parquet
‚îÇ
‚îú‚îÄ‚îÄ üìÅ gold/                      # Capa Gold (agregaciones)
‚îÇ   ‚îî‚îÄ‚îÄ ventas/                   # (Futuro: para Equipo B)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                   # Scripts de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ ingesta_bronze.py        # Ingesta Bronze Layer
‚îÇ   ‚îî‚îÄ‚îÄ limpieza_silver.py       # Limpieza Silver Layer
‚îÇ
‚îú‚îÄ‚îÄ üìÅ logs/                      # Logs de ejecuci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ main_workflow.log
‚îÇ   ‚îú‚îÄ‚îÄ ingesta_bronze.log
‚îÇ   ‚îî‚îÄ‚îÄ limpieza_silver.log
‚îÇ
‚îú‚îÄ‚îÄ üìÅ config/                    # Configuraciones
‚îÇ
‚îú‚îÄ‚îÄ üìÑ main.py                    # Workflow principal
‚îú‚îÄ‚îÄ üìÑ requirements.txt           # Dependencias Python
‚îú‚îÄ‚îÄ üìÑ ARQUITECTURA.md           # Documentaci√≥n t√©cnica
‚îú‚îÄ‚îÄ üìÑ README.md                 # Este archivo
‚îú‚îÄ‚îÄ üìÑ .gitignore
‚îÇ
‚îî‚îÄ‚îÄ üìä Archivos de datos (inputs)
    ‚îú‚îÄ‚îÄ clientes_extra.txt       # 500 registros
    ‚îú‚îÄ‚îÄ clientes_info.csv        # 500 registros
    ‚îî‚îÄ‚îÄ clientes.sql             # 500 registros
```

---

##  Pipeline de Datos

### Etapa 1: Ingesta Bronze Layer

**Script:** `scripts/ingesta_bronze.py`

**Proceso:**
1. Leer archivos TXT, CSV y SQL
2. Validar estructura y tipos de datos
3. Convertir a formato Parquet
4. Guardar en `/bronze/ventas/`
5. Generar estad√≠sticas de ingesta

**Validaciones Implementadas:**

| Campo | Validaci√≥n | Acci√≥n |
|-------|-----------|--------|
| `codigo` | Num√©rico, no nulo | Drop si inv√°lido |
| `tipo_servicio` | APP \| LOCAL \| AMBOS | Warning si diferente |
| `codigo_unico` | Formato XXXX99 | Warning si inv√°lido |
| `fecha_afiliacion` | Fecha v√°lida | Coerce o null |
| `promedio_compras` | >= 0 | Coerce o 0 |
| `tipo_cliente` | 1-5 | Filter fuera de rango |

**Ejecutar solo ingesta:**
```bash
python scripts/ingesta_bronze.py
```

---

### Etapa 2: Limpieza Silver Layer

**Script:** `scripts/limpieza_silver.py`

**Proceso:**
1. Leer datos de Bronze
2. Normalizar texto (trim, upper, lower)
3. Estandarizar fechas (YYYY-MM-DD)
4. Manejar valores nulos
5. Aplicar reglas de negocio
6. Guardar en `/silver/ventas/`

**Transformaciones:**

| Transformaci√≥n | Descripci√≥n | Ejemplo |
|----------------|-------------|---------|
| **Normalizaci√≥n de texto** | Upper para c√≥digos, Proper case para nombres | "XMOR34", "Felipe Fuentes" |
| **Limpieza de fechas** | Trim + formato YYYY-MM-DD | " 2025-01-06" ‚Üí "2025-01-06" |
| **RUT chileno** | Eliminar puntos y guiones | "1.931.858-3" ‚Üí "19318583" |
| **Manejo de nulos** | Fill con valores default o drop | null ‚Üí "No Aplica" |
| **Validaci√≥n de rangos** | Filtrar valores fuera de rango | tipo_cliente: 1-5 |

**Ejecutar solo limpieza:**
```bash
python scripts/limpieza_silver.py
```

---

## üîç Verificaci√≥n de Resultados

### 1. Verificar Archivos Generados

```bash
# Ver estructura completa
tree -L 3

# Ver tama√±o de archivos
du -sh bronze/ silver/ gold/

# Contar archivos Parquet
find . -name "*.parquet" | wc -l
```

### 2. Inspeccionar Datos con Python

```python
import pandas as pd

# Leer datos de Silver
clientes = pd.read_parquet('silver/ventas/clientes_silver.parquet')
info = pd.read_parquet('silver/ventas/clientes_info_silver.parquet')
extra = pd.read_parquet('silver/ventas/clientes_extra_silver.parquet')

# Explorar datos
print(f"Total clientes: {len(clientes)}")
print(f"\nPrimeras 5 filas:")
print(clientes.head())

print(f"\nInfo del dataset:")
print(clientes.info())

print(f"\nEstad√≠sticas descriptivas:")
print(info.describe())
```

### 3. Verificar Calidad de Datos

```bash
# Script de verificaci√≥n
python << 'PYEOF'
import pandas as pd

# Leer Silver
clientes = pd.read_parquet('silver/ventas/clientes_silver.parquet')
info = pd.read_parquet('silver/ventas/clientes_info_silver.parquet')
extra = pd.read_parquet('silver/ventas/clientes_extra_silver.parquet')

print("="*60)
print("üìä REPORTE DE CALIDAD DE DATOS")
print("="*60)

print("\n1Ô∏è‚É£ CLIENTES (Demogr√°ficos)")
print(f"   Registros: {len(clientes)}")
print(f"   Nulos por columna:")
print(clientes.isnull().sum())

print("\n2Ô∏è‚É£ CLIENTES INFO (M√©tricas)")
print(f"   Registros: {len(info)}")
print(f"   Promedio compras: ${info['promedio_compras'].mean():,.0f}")
print(f"   Tiempo promedio: {info['tiempo_permanencia_min'].mean():.1f} min")

print("\n3Ô∏è‚É£ CLIENTES EXTRA (Afiliaci√≥n)")
print(f"   Registros: {len(extra)}")
print(f"   Tipos de servicio:")
print(extra['tipo_servicio'].value_counts())

print("\n‚úÖ Verificaci√≥n completada")
PYEOF
```

### 4. Visualizar Datos en Jupyter (Opcional)

```bash
# Instalar Jupyter
pip install jupyter pandas matplotlib seaborn

# Iniciar Jupyter Notebook
jupyter notebook
```

Crea un nuevo notebook y ejecuta:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Leer datos
info = pd.read_parquet('silver/ventas/clientes_info_silver.parquet')

# Gr√°ficos
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Distribuci√≥n de tipo de cliente
info['tipo_cliente'].value_counts().plot(kind='bar', ax=axes[0,0])
axes[0,0].set_title('Distribuci√≥n Tipo de Cliente')

# 2. Promedio de compras
info['promedio_compras'].hist(bins=50, ax=axes[0,1])
axes[0,1].set_title('Distribuci√≥n Promedio de Compras')

# 3. Tiempo de permanencia
info['tiempo_permanencia_min'].hist(bins=50, ax=axes[1,0])
axes[1,0].set_title('Tiempo de Permanencia')

# 4. Tipo de alimentaci√≥n
info['tipo_alimentacion'].value_counts().plot(kind='pie', ax=axes[1,1])
axes[1,1].set_title('Tipo de Alimentaci√≥n')

plt.tight_layout()
plt.show()
```

---

## üõ†Ô∏è Soluci√≥n de Problemas

### Problema: Error de Spark al parsear fechas

**S√≠ntoma:**
```
DateTimeParseException: Text ' 2025-01-06' could not be parsed
```

**Soluci√≥n:**
El script ya incluye `trim()` para limpiar espacios. Si persiste:

```bash
# Ejecutar fix de fechas
python << 'EOF'
import pandas as pd
df = pd.read_parquet('bronze/ventas/clientes_extra_bronze.parquet')
df['fecha_afiliacion'] = df['fecha_afiliacion'].str.strip()
df.to_parquet('bronze/ventas/clientes_extra_bronze.parquet', index=False)
print("‚úì Fechas corregidas")
EOF

# Re-ejecutar pipeline
python main.py
```

### Problema: Virtual environment no activo

**S√≠ntoma:**
```bash
(venv) no aparece en el prompt
```

**Soluci√≥n:**
```bash
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\Activate.ps1  # Windows
```

### Problema: ModuleNotFoundError

**S√≠ntoma:**
```
ModuleNotFoundError: No module named 'pyspark'
```

**Soluci√≥n:**
```bash
pip install -r requirements.txt --force-reinstall
```

---

##  Schema de Datos Silver

### clientes_silver.parquet

| Columna | Tipo | Descripci√≥n | Ejemplo |
|---------|------|-------------|---------|
| `codigo` | int | ID √∫nico del cliente | 1 |
| `nombre` | string | Nombre normalizado | "Felipe" |
| `apellido` | string | Apellido normalizado | "Fuentes" |
| `comuna` | string | Comuna/ciudad | "Vitacura" |
| `rut` | string | RUT sin formato | "19318583" |
| `fecha_nacimiento` | date | Fecha de nacimiento | 1971-10-04 |
| `religion` | string | Religi√≥n | "Atea" |
| `processed_at` | timestamp | Fecha de procesamiento | 2025-12-05 01:08:25 |

### clientes_info_silver.parquet

| Columna | Tipo | Descripci√≥n | Ejemplo |
|---------|------|-------------|---------|
| `codigo_cliente` | int | ID del cliente | 1 |
| `tarjeta_beneficios` | string | SI/NO | "SI" |
| `tipo_cliente` | int | Categor√≠a 1-5 | 3 |
| `promedio_compras` | double | Promedio en CLP | 397192.0 |
| `tipo_alimentacion` | string | Dieta | "normal" |
| `tiempo_permanencia_min` | int | Minutos en tienda | 15 |
| `processed_at` | timestamp | Fecha de procesamiento | 2025-12-05 01:08:25 |

### clientes_extra_silver.parquet

| Columna | Tipo | Descripci√≥n | Ejemplo |
|---------|------|-------------|---------|
| `codigo` | int | ID del cliente | 1 |
| `tipo_servicio` | string | APP/LOCAL/AMBOS | "APP" |
| `codigo_unico` | string | C√≥digo de afiliaci√≥n | "XMOR34" |
| `fecha_afiliacion` | date | Fecha de afiliaci√≥n | 2025-01-06 |
| `processed_at` | timestamp | Fecha de procesamiento | 2025-12-05 01:08:25 |

---

## Equipo

### Equipo A - Ingenier√≠a de Datos (Bronze & Silver)

| Nombre | Rol | Responsabilidad |
|--------|-----|-----------------|
| **William Moncada** | Data Engineer | Arquitectura y dise√±o del pipeline |
| **Matias Osores** | Data Engineer | Implementaci√≥n Bronze Layer |
| **Catalina Fajardo** | Data Engineer | Implementaci√≥n Silver Layer |
| **Dilan Ferreira** | Data Engineer | Validaciones y testing |
| **Jean Pierre Trinidad** | Data Engineer | Documentaci√≥n y DevOps |

### Equipo B - An√°lisis de Datos (Silver & Gold)

Responsables del an√°lisis exploratorio, modelado predictivo y generaci√≥n de insights de negocio a partir de los datos en Silver Layer.

---

##  Documentaci√≥n Adicional

- **[ARQUITECTURA.md](ARQUITECTURA.md)**: Documentaci√≥n t√©cnica detallada
- **[Logs](logs/)**: Logs de ejecuci√≥n del pipeline
- **[Scripts](scripts/)**: C√≥digo fuente comentado

---

## üîó Referencias

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [GitHub Codespaces](https://github.com/features/codespaces)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

---

## Licencia

Este proyecto es parte de un caso de estudio acad√©mico para LIDL Chile.

---

##  Contribuciones

Para reportar problemas o sugerir mejoras:

1. Abre un Issue en GitHub
2. Describe el problema detalladamente
3. Incluye logs relevantes
4. Prop√≥n una soluci√≥n si es posible

---

## Contacto

Para consultas sobre el proyecto:
- **Email**: jean.trinidad@inacapmail.cl
- **Slack**: #lidl-data-engineering

---

<div align="center">

**Proyecto LIDL Chile - Data Engineering Team**

*Construyendo el futuro del retail con datos de calidad*

[‚¨ÜÔ∏è Volver arriba](#-proyecto-lidl---pipeline-de-datos-de-clientes)

</div>
